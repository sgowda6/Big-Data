{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of HW2 (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_fp7u5dtp6t"
      },
      "source": [
        "**Install Spark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TlZ7WgdClk7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJdWQ66cttsC"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.mirrors.pair.com/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "\n",
        "!tar -xvf spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy3-OGBKukUU"
      },
      "source": [
        "**Task 1 - Step 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mf2x_XyutQa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "841dd38c-603f-4c68-8c34-156b097b8be1"
      },
      "source": [
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from pyspark import SparkContext\n",
        "import datetime\n",
        "import pyspark as sp\n",
        "\n",
        "\n",
        "sc = SparkContext\n",
        "\n",
        "\n",
        "nSlices = 10\n",
        "sc = sp.SparkContext\n",
        "rdd1 = sc.parallelize([1],nSlices)\n",
        "\n",
        "# [...]\n",
        "sc.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-63cb1daab36c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtabulate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtabulate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH4Nvc18eeox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff44b42a-5453-4bcb-d0a7-d7e1b7ccedd2"
      },
      "source": [
        "#read CSV into a DF\n",
        "df = spark.read.format('csv').options(header='true', multiline='true').load('Amazon_Responded_Oct05.csv')\n",
        "\n",
        "#df.show()\n",
        "df.count()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "397583"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cChKi4a_tIpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8d54e6-88f3-438c-ede7-43a4a24bc8df"
      },
      "source": [
        "#include only the six columns needed \n",
        "df = df[['id_str','tweet_created_at','user_verified','favorite_count','retweet_count','text_']]\n",
        "\n",
        "#Drop na values\n",
        "df = df.dropna()\n",
        "\n",
        "df.count()\n",
        "#df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "348193"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSTCwSAYGYNd"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession, functions as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP5y71MfCCKg"
      },
      "source": [
        "#Create a new column which contains month and date \n",
        "df = df.withColumn(\"date\", F.regexp_extract(df.tweet_created_at, r'(\\b\\w{3} \\d{2})',0))\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdfCu-g1DvwC"
      },
      "source": [
        "#Step 1 - Remove the records where “user_verified” is “FALSE” i.e include records which is equal to True\n",
        "df = df[df.user_verified == \"True\"]\n",
        "\n",
        "df.show()\n",
        "df.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47UfJUYMnz0a"
      },
      "source": [
        "**Task 1 - Step 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpJT6K0hAEMS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSL3rEYOpYYV"
      },
      "source": [
        "#convert the df to RDD\n",
        "rdd = df.rdd\n",
        "rdd.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIxUMsWhGkjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed1cd88c-6f3e-4030-9820-fec8414a1f02"
      },
      "source": [
        "step2 = rdd.map(lambda x: (x[6], x[4])).groupByKey().mapValues(lambda vals: len(vals)).collect()\n",
        "\n",
        "number_of_tweets_per_day = spark.createDataFrame(step2)\n",
        "\n",
        "number_of_tweets_per_day.orderBy('_2', ascending=False).show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----+\n",
            "|    _1|  _2|\n",
            "+------+----+\n",
            "|Jan 03|1536|\n",
            "|Jan 10|1508|\n",
            "|Jan 11|1496|\n",
            "|Jan 12|1409|\n",
            "|Jan 06|1363|\n",
            "|Jan 07|1360|\n",
            "|Jan 20|1342|\n",
            "|Mar 02|1298|\n",
            "|Jan 13|1295|\n",
            "|Jan 21|1292|\n",
            "+------+----+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WhIDwOACpTD"
      },
      "source": [
        "Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3HZEwoHddr_"
      },
      "source": [
        "Amazon Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MWlKBtwHgvT"
      },
      "source": [
        "#include only the two columns needed \n",
        "df2 = df[['id_str','text_']]\n",
        "df2.show()\n",
        "df2.count()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0whzHWYdjSu"
      },
      "source": [
        "Find Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp3gF0BFIJ8i"
      },
      "source": [
        "#read CSV into a DF\n",
        "find_text=spark.read.format('csv').options(header='true', multiline='true').load('find_text.csv')\n",
        "find_text.show()\n",
        "find_text.count()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8hDNZcldm38"
      },
      "source": [
        " Convert to RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlhOAqp6H9Nc"
      },
      "source": [
        "Amazon=df2.rdd\n",
        "find_text1=find_text.rdd\n",
        "find_text1.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7yWaDvXNsn4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JppQIZu5dqD9"
      },
      "source": [
        "Join "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT5ANBNrnx2D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnJZTj43Jr9g"
      },
      "source": [
        "op=Amazon.join(find_text)\n",
        "#x=Amazon\n",
        "#y=find_text1\n",
        "#a=sorted(x.leftOuterJoin(y).collect())\n",
        "#stopwords = ['None']\n",
        "#rdd3 = a.filter(lambda x: x not in stopwords)\n",
        "#a=(list(filter(lambda x: x[0]>0,x[2] > 0)))\n",
        "#a=(list(filter(None)))\n",
        "\n",
        "\n",
        "#op.collect()\n",
        "#op=Amazon.map((lambda x:(x[0]),(x[1])).leftjoin(find_text(lambda x:(x[0]),x[1]))).collect()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJOP2Q2nd142"
      },
      "source": [
        "Convert to Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8mzyrHlcIon"
      },
      "source": [
        "from pyspark.sql.types import*\n",
        "schema = StructType([StructField(\"id_str\", StringType(), True), StructField(\"text\", StringType(), True)])\n",
        "DF1=spark.createDataFrame(op,schema=schema)\n",
        "DF1.show()\n",
        "\n",
        "\n",
        "#df = spark.createDataFrame(a,['idx', 'tuple'],schema=schema)\n",
        "#df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Hqf-Jrd6ms"
      },
      "source": [
        "Convert to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw53eBp6dbiB"
      },
      "source": [
        "DF1.toPandas().to_csv('FIND_TEXT3.csv') #refresh and save the text file\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh30NBCJPnbi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SHVukL8SyWO"
      },
      "source": [
        "DF1.toPandas().to_csv('FIND_TEXT1.csv')\n",
        "#DF1.write.format('com.databricks.spark.csv').save('FIND_TEXT.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL1uN6Vc73ta"
      },
      "source": [
        "dfjoin=df2.join(find_text, df2.id_str==find_text.id_str, how='left')\n",
        "dfjoin.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}